{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra functions for lr_find\n",
    "class ParameterModule(nn.Module):\n",
    "    \"Register a lone parameter 'p' in a module\"\n",
    "    def __init__(self, p:nn.Parameter):\n",
    "        super().__init__()\n",
    "        self.val = p\n",
    "    def forward(self, x): return x\n",
    "\n",
    "def children_and_parameters(m:nn.Module):\n",
    "    \"Return the children of `m` and its direct parameters not registered in modules.\"\n",
    "    children = list(m.children())\n",
    "    children_p = sum([[id(p) for p in c.parameters()] for c in m.children()],[])\n",
    "    for p in m.parameters():\n",
    "        if id(p) not in children_p: children.append(ParameterModule(p))\n",
    "    return children\n",
    "\n",
    "flatten_model = lambda m: sum(map(flatten_model,children_and_parameters(m)),[]) if len(list(m.children())) else [m]\n",
    "\n",
    "# lr_range\n",
    "def lr_range(model, lr):\n",
    "    \"\"\"\n",
    "    Build differential learning rate from lr\n",
    "    Arguments:\n",
    "        lr :- float or slice\n",
    "        num_layer :- number of layers with requires_grad=True\n",
    "    Returns:\n",
    "        Depending upon lr\n",
    "    \"\"\"\n",
    "    if not isinstance(lr, slice): \n",
    "        return lr\n",
    "    \n",
    "    num_layer = [nn.Sequential(*flatten_model(model))]\n",
    "    if lr.start: \n",
    "        mult = lr.stop / lr.start\n",
    "        step = mult**(1/(num_layer-1))\n",
    "        res = np.array([lr.start*(step**i) for i in range(num_layer)])\n",
    "    else:\n",
    "        res = [lr.stop/10]*(num_layer-1) + [lr.stop]\n",
    "    \n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_find\n",
    "def lr_find(model:nn.Module, data, start_lr=1e-7, end_lr=10, num_it=100, stop_div:bool=True, wd:float=None):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model ->\n",
    "        start_lr -> lr at which cyclic lr should start\n",
    "        end_lr -> lr at which cyclic lr shoudl end\n",
    "        num_it -> number of batches you want to run\n",
    "        stop_div -> if loss diverges, stop\n",
    "        wd ->\n",
    "        data -> your train data_loader (of class torch.utils.data.DataLoader)\n",
    "        \n",
    "        If I have 100 images and I make a batch of 10 images, so num_batches=100/10 = 10\n",
    "    \"\"\"\n",
    "    start_lr = lr_range(model, start_lr)\n",
    "    start_lr = np.array(start_lr) if isinstance(start_lr, (tuple, list)) else start_lr\n",
    "    \n",
    "    end_lr = lr_range(end_lr)\n",
    "    end_lr = np.array(end_lr) if isinstance(end_lr, (tuple, list)) else end_lr\n",
    "    \n",
    "    cb = LRFinder(model, start_lr, end_lr, num_it, stop_div)\n",
    "    epochs = int(np.ceil(num_it/len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annealing_no(start, end, pct:float):\n",
    "    \"No annealing, always return `start`.\"\n",
    "    return start\n",
    "def annealing_linear(start, end, pct:float):\n",
    "    \"Linearly anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n",
    "    return start + pct * (end-start)\n",
    "def annealing_exp(start, end, pct:float):\n",
    "    \"Exponentially anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n",
    "    return start * (end/start) ** pct\n",
    "def annealing_cos(start, end, pct:float):\n",
    "    \"Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n",
    "    cos_out = np.cos(np.pi * pct) + 1\n",
    "    return end + (start-end)/2 * cos_out\n",
    "def do_annealing_poly(start, end, pct:float, degree):\n",
    "    return end + (start-end) * (1-pct)**degree\n",
    "\n",
    "class Stepper():\n",
    "    \"Used to \\\"step\\\" from start, end ('vals') over 'n_iter' iterations on a schedule\"\n",
    "    def __init__(self, vals, n_iter:int, func=None):\n",
    "        self.start, self.end = (vals[0], vals[1]) if isinstance(vals, tuple) else (vals,0)\n",
    "        self.n_iter = max(1, n_iter)\n",
    "        if func is None:\n",
    "            self.func = annealing_linear if isinstance(vals, tuple) else annealing_no\n",
    "        else:\n",
    "            self.func = func\n",
    "        self.n = 0\n",
    "    \n",
    "    def step(self):\n",
    "        \"Return next value along annealed schedule\"\n",
    "        self.n += 1\n",
    "        return self.func(self.start, self.end, self.n/self.n_iter)\n",
    "        \n",
    "    @property\n",
    "    def is_done(self)->bool:\n",
    "        \"Return 'True' if schedule completed\"\n",
    "        return self.n >= self.n_iter\n",
    "        \n",
    "# def save_model_dict(path, name, model:nn.Module, optimizer, with_opt:bool=True):\n",
    "#     \"\"\"\n",
    "#     Arguments:-\n",
    "#         path -> Where you want to save the files\n",
    "#                 from pathlib import Path\n",
    "#                 path = Path('/home/kushaj/Desktop')\n",
    "#         name -> Name of the file\n",
    "#         with_opt -> if True then optimizer state is also saved\n",
    "#     \"\"\"\n",
    "#     path = path/f'{name}.pth' \n",
    "#     if not with_opt:\n",
    "#         state = model.state_dict()\n",
    "#     else:\n",
    "#         state = {'model': model.state_dict(),\n",
    "#                  'opt': optimizer.state_dict()}\n",
    "#     torch.save(state, path)\n",
    "        \n",
    "def LRFinder(model, \n",
    "             data, \n",
    "             loss_fn, \n",
    "             opt, \n",
    "             wd, \n",
    "             start_lr:float=1e-7, \n",
    "             end_lr:float=10, \n",
    "             num_it:int=100, \n",
    "             stop_div:bool=True):\n",
    "    sched = Stepper((start_lr, end_lr), num_it, annealing_exp)\n",
    "    # save model_dict\n",
    "    model_state = model.state_dict()\n",
    "    opt_state = opt.state_dict()\n",
    "    \n",
    "    opt.lr = sched.start\n",
    "    stop = False\n",
    "    best_loss = 0.\n",
    "    flag = False\n",
    "    losses = []\n",
    "    iteration = 0\n",
    "    lrs = []\n",
    "    moms = []\n",
    "    \n",
    "    while True:\n",
    "        for dat in data:\n",
    "            # Batch begin\n",
    "            lrs.append(opt.lr)\n",
    "            moms.append(opt.mom)\n",
    "            \n",
    "            inputs, labels = dat\n",
    "            opt.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            # For adamW\n",
    "            for group in opt.param_groups():\n",
    "                for param in group['params']:\n",
    "                    param.data = param.data.add(-wd * group['lr'], param.data)\n",
    "                    \n",
    "            opt.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            opt.lr = sched.step()\n",
    "            if iteration == 0 or loss < best_loss:\n",
    "                best_loss = loss\n",
    "            iteration += 1\n",
    "            \n",
    "            if sched.is_done or (stop_div and (loss > 4*best_loss or torch.isnan(smooth_loss))):\n",
    "                flag = True\n",
    "                break\n",
    "        \n",
    "        if flag:\n",
    "            break\n",
    "    \n",
    "    # Load state back\n",
    "    model.load_state_dict(model_state)\n",
    "    opt.load_state_dict(opt_state)\n",
    "    \n",
    "    print('LR Finder is complete')\n",
    "    \n",
    "    return losses, lrs\n",
    "\n",
    "def plot_lr_finder(losses, \n",
    "                   lrs, \n",
    "                   skip_start:int=10, \n",
    "                   skip_end:int=5, \n",
    "                   suggestion:bool=False, \n",
    "                   return_fig:bool=None, \n",
    "                   smoothen_by_spline:bool=True):\n",
    "    lrs = lrs[skip_start:-skip_end] if skip_end > 0 else lrs[skip_start:]\n",
    "    losses = losses[skip_start:-skip_end] if skip_end > 0 else losses[skip_start:]\n",
    "    if smoothen_by_spline:\n",
    "        xs = np.arange(len(losses))\n",
    "        spl = UnivariateSpline(xs, losses)\n",
    "        losses = spl(xs)\n",
    "        \n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.plot(lrs, losses)\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_xlabel(\"Learning Rate\")\n",
    "    ax.set_xscale('log')\n",
    "    ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%.0e'))\n",
    "    if suggestion:\n",
    "        try:\n",
    "            mg = (np.gradient(np.array(losses))).argmin()\n",
    "        except:\n",
    "            print(\"Failed to compute the gradients, there might not be enough points.\")\n",
    "            return\n",
    "        print(\"Min numerical gradient: {lrs[mg]:.2E}\")\n",
    "        ax.plot(lrs[mg], losses[mg], markersize=10, marker='o', color='red')\n",
    "    \n",
    "    if return_fig is not None:\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Data/cifar10/train/'\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(path, transform=transform)\n",
    "\n",
    "data = torch.utils.data.DataLoader(trainset, batch_size=512, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy' has no attribute 'interpolate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ec20453c5311>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'scipy' has no attribute 'interpolate'"
     ]
    }
   ],
   "source": [
    "scipy.interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
